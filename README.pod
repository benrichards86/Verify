<html><head><title></title>
<meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1" >
</head>
<body class='pod'>
<!--
  generated by Pod::Simple::HTML v3.22,
  using Pod::Simple::PullParser v3.22,
  under Perl v5.014002 at Mon Jul 22 18:11:41 2013 GMT.

 If you want to change this HTML document, you probably shouldn't do that
   by changing it directly.  Instead, see about changing the calling options
   to Pod::Simple::HTML, and/or subclassing Pod::Simple::HTML,
   then reconverting this document from the Pod source.
   When in doubt, email the author of Pod::Simple::HTML for advice.
   See 'perldoc Pod::Simple::HTML' for more info.

-->

<!-- start doc -->
<a name='___top' class='dummyTopAnchor' ></a>

<h1><a class='u'
name="NAME"
>NAME</a></h1>

<p>Verify - A simple verification tool to manage the definition,
building,
and running of tests in a verification environment.</p>

<h1><a class='u'
name="SYNOPSIS"
>SYNOPSIS</a></h1>

<p>verify [options] <i>config</i>::<i>test</i>[,<i>params</i>...] ...</p>

<pre> Options:
   -h, --help             Print usage information
   --man                  Display manpage-styled help
   --man2html             Writes manpage-styled help to a formatted HTML file
   -l, --log              Specifies log filename
   -d, --debug            Use debug mode
   -i, --interactive      Interactive debug mode
   -n                     Just print what would have run without running anything
   -e, --email            Email Verify status
   -q, --quiet            Quiet output
   -v, --verbose          Verbose output
   -p, --print            List available tests
   -P, --parallel         Run in parallel mode
   -r, --report           Enable test(s) pass/fail report logging
   --no-build             Skip build
   --build                Enable build
   --no-run               Skip run
   --run                  Enable run
   --version              Display version information</pre>

<h1><a class='u'
name="OPTIONS"
>OPTIONS</a></h1>

<dl>
<dt><a name="-h"
>-h</a></dt>

<dd>
<dt><a name="--help"
>--help</a></dt>

<dd>
<p>Shows this usage information.</p>

<dt><a name="--man"
>--man</a></dt>

<dd>
<p>Displays a manpage-styled help documentation.</p>

<dt><a name="--man2html"
>--man2html</a></dt>

<dd>
<p>Writes the same manpage-styled help displayed by <code>--man</code> into an HTML-formatted file called <em>verify.html</em>.</p>

<dt><a name="-l"
>-l</a></dt>

<dd>
<dt><a name="--log"
>--log</a></dt>

<dd>
<p>Specifies the name of the log file. Default is <em>verify.log</em>.</p>

<dt><a name="-d"
>-d</a></dt>

<dd>
<dt><a name="--debug"
>--debug</a></dt>

<dd>
<p>Enables debugging mode for the test(s). (Requires implementation in <em>build_test.pm</em> and <em>run_test.pm</em>.)</p>

<dt><a name="-i"
>-i</a></dt>

<dd>
<dt><a name="--interactive"
>--interactive</a></dt>

<dd>
<p>Enables debugging in interactive mode for the test(s). (Requires implementation in <em>build_test.pm</em> and <em>run_test.pm</em>.)</p>

<dt><a name="-n"
>-n</a></dt>

<dd>
<p>Only print out what the tool would have done, without actually running anything in the shell.</p>

<dt><a name="--no-build"
>--no-build</a></dt>

<dd>
<p>Skips the build step.</p>

<dt><a name="--build"
>--build</a></dt>

<dd>
<p>Enables the build step (default). This will override previous usage of <code>--no-build</code>.</p>

<dt><a name="--no-run"
>--no-run</a></dt>

<dd>
<p>Skips the run step.</p>

<dt><a name="--run"
>--run</a></dt>

<dd>
<p>Enables the run step (default). This will override previous usage of <code>--no-run</code>.</p>

<dt><a name="-e"
>-e</a></dt>

<dd>
<dt><a name="--email"
>--email</a></dt>

<dd>
<p>Email results to one or more email recipients at tool completion.</p>

<dt><a name="-q"
>-q</a></dt>

<dd>
<dt><a name="--quiet"
>--quiet</a></dt>

<dd>
<p>Suppresses all console output.</p>

<dt><a name="-v"
>-v</a></dt>

<dd>
<dt><a name="--verbose"
>--verbose</a></dt>

<dd>
<p>Enables console output (default). This will override previous usage of <code>--quiet</code>.</p>

<dt><a name="-p"
>-p</a></dt>

<dd>
<dt><a name="--print"
>--print</a></dt>

<dd>
<p>Causes Verify to find and list all available tests, and then exits.</p>

<dt><a name="-P"
>-P</a></dt>

<dd>
<dt><a name="--parallel"
>--parallel</a></dt>

<dd>
<p>Enables running of multiple tests in parallel. See the section called <b>Running Tests</b> in the manual for more information.</p>

<dt><a name="-r"
>-r</a></dt>

<dd>
<dt><a name="--report"
>--report</a></dt>

<dd>
<p>When used, pass/fail status of all tests run will be recorded in a tab-delimited file <em>report.txt</em>.</p>

<dt><a name="--version"
>--version</a></dt>

<dd>
<p>Displays version information for Verify on the console.</p>
</dd>
</dl>

<h1><a class='u'
name="DESCRIPTION"
>DESCRIPTION</a></h1>

<h2><a class='u'
name="Files,_Installing,_Configuring"
>Files, Installing, Configuring</a></h2>

<h3><a class='u'
name="Required_files"
>Required files</a></h3>

<p>This script was designed with portability in mind, so there are only a few files needed for basic script operation. The initial configuration step when installing this script is also very minimal. The whole of this script consists of at least three groups of files: the main source, indexing and parsing modules, and user-implemented modules for the build and run steps.</p>

<p>The main source is in <em>verify.pl</em>. It contains all the code necessary for organizing and invoking tests. It will query the test file indexer (by default, <em>TestIndex.pm</em>) and invoke the build and run steps from methods implemented in <em>build_test.pm</em> and <em>run_test.pm</em>, respectively. These two files may exist anywhere, so long as they are both in the same directory and that you configure <em>verify.pl</em> to know where these files are. I will touch on configuration in the next subsection.</p>

<p><em>TestIndex.pm</em> contains the code necessary for resolving <code>config::testname</code> pairs into its testfile. It utilizes test file parsing functions from <em>TestFileParser.pm</em> to store the test definition into a hash variable. It also contains the code necessary for outputting the entire test list (if the tool is invoked with <code>-p</code>).</p>

<p><em>build_test.pm</em> contains the code necessary for your build step. It implements three functions: <code>pre_build()</code>, <code>build()</code>, and <code>post_build()</code>. The implementation of these functions is not provided, because they are to be written by the user. <em>run_test.pm</em> is similar, but is associated with the run step. It implements three similar functions: <code>pre_run()</code>, <code>run()</code>, and <code>post_run()</code>.</p>

<h3><a class='u'
name="Installation_and_Configuration"
>Installation and Configuration</a></h3>

<p>To install this tool, simply store <em>verify.pl</em> wherever you deem appropriate. Implement your <em>build_test.pm</em> and <em>run_test.pm</em> files and store those somewhere appropriate, as well. (Details for implementation of these files will be provided in the next subsection). This script requires a single environment variable to be set: <code>$PRJ_HOME</code>. This points to the head of your verification environment.</p>

<p>Next, you have to tell the tool where your <em>build_test.pm</em> and <em>run_test.pm</em> files are, as well as where it should start looking for test files. To do this, create a file called <em>verify.ini</em> and place it in <code>$PRJ_HOME</code>. Open it in a text editor and any of the following:</p>

<pre>    testsdir=./path/to/tests
    libsdir=./path/to/libs</pre>

<p><code>testsdir</code> will specify the <i>relative</i> path to your test files. It will cause the tool to start under <code>$PRJ_HOME/testsdir</code> and recursively search for test definitions files. <code>libsdir</code> tells the script where to look for your <em>build_test.pm</em> and <em>run_test.pm</em> files. This is also a relative path, under <code>$PRJ_HOME</code>.</p>

<p>Your <em>verify.ini</em> file can exclude either of these lines. Whatever the tool does not read in from this file, it will use the default.</p>

<p><b>NOTE:</b> This tool will <i>always</i> look for the <em>verify.ini</em> file in <code>$PRJ_HOME</code>. If it does not exist, it will default <code>testsdir</code> to <code>$PRJ_HOME/verification/tests</code> and <code>libsdir</code> to <code>$PRJ_HOME/verification/scripts</code>. If there are any errors when it tries to pull in these modules (either due to compilation or file not found), it will cause this tool to die, logging out the error.</p>

<h3><a class='u'
name="Implementing_the_Build_and_Run_Steps"
>Implementing the Build and Run Steps</a></h3>

<p>Templates for these two files are provided with the source code of this tool. You will notice that they are very sparse. They only contain three empty subroutine definitions, each. For both <code>build()</code> and <code>run()</code>, you will notice that there is an associated <code>pre_*()</code> and <code>post_*()</code> subroutine associated. As you might guess, these subroutines will be invoked before and after its corresponding method, respectively. You don&#39;t have to implement these functions, but they must exist. Otherwise, the Perl compiler will fail. I recommend using them for steps that are required as part of your build and run steps, but aren&#39;t directly involved in actually building or running the test itself. For instance, I use <code>pre_build()</code> and <code>pre_run()</code> to parse test parameters, and use <code>post_run()</code> to allow me to invoke my debugging environment after a test completes, if desired. However, you can use them to do anything you&#39;d like.</p>

<p>When implementing these functions, you should not use Perl&#39;s built-in <code>print()</code> variants for displaying output to standard output. As there is an extensive output and logging mechanism built into the tool, anything you report using Perl&#39;s built-in functions will not be able to be captured in the log files. Instead, I provide a function for you to use instead:</p>

<pre>    verify::tlog(level, message);</pre>

<p>This function will display a message to your console, as well as log it. <code>message</code> can be any string. <code>level</code> can be either <code>0</code> or <code>1</code>. These level codes correspond to standard output and standard error, respectively. The power of this function is that if you run the test in regression mode (with <code>--parallel</code>), or decide to force any console output to be suppressed (with <code>--quiet</code>), any calls to <code>tlog()</code> will suppress console output, but still output to the logfiles for reference later. Also, anything that you output using <code>tlog()</code> with a level of <code>1</code> will be both written to the log file, as well as a dedicated error log (named <em>logfile.log.error</em>). This can be useful if you want to isolate errors for debug purposes.</p>

<p>Similarly, instead of using Perl&#39;s built-in <code>system()</code> call for invoking shell commands, I provide another subroutine for you to use:</p>

<pre>    verify::run_command(command_str);</pre>

<p>This function behaves just as Perl&#39;s built-in <code>system()</code> call, except it has a few additional capabilities. It will execute your command (stored in <code>command_str</code>), retrieve the error code and return it back to the caller. However, it also will capture all output from the console and redirect it to the log file. Anything sent by the shell command to standard output and standard error will be displayed on the console and written to the log. It also has the capability to suppress console output in the same way as <code>tlog()</code> provides. For logging purposes, it will also log the command that is being executed in the subshell. As an additional benefit, if you invoke the tool using <code>-n</code> (&#34;null&#34; mode), it will cause the tool to follow all steps in a normal run but skip actual execution of your build and run tools. Using this command-line switch will tell <code>verify::run_command()</code> to simply output the command it would have run, and return without actually running it.</p>

<p>As before, if you invoke any commands using Perl&#39;s built-in <code>system()</code> call, you will not capture any of the test&#39;s output to the log files, nor be able to suppress console output. It also will not be able to skip running the command if you decide to invoke the tool using <code>-n</code>.</p>

<p>Lastly, I have a replacement for Perl&#39;s built-in function, <code>die()</code>. This will cause the tool to immediately exit, but I want to be able to control how the tool exits so that even in a case where a <code>die()</code> call is appropriate, we will have potentially important information logged correctly. Instead of calling <code>die()</code>, use:</p>

<pre>    verify::tdie(message);</pre>

<p>It does the same thing as Perl&#39;s <code>die()</code>, but allows capturing of messages to our log files, and emailing results (if you enabled emailing when you invoked the tool).</p>

<p>Of course, I do not prevent you from using Perl&#39;s built-in <code>print()</code> variants, nor <code>system()</code> (and related). You may use them if you feel it would be useful. However, if you choose to do so, I believe you should be aware of the caveats of doing so.</p>

<h2><a class='u'
name="Running_Tests"
>Running Tests</a></h2>

<h3><a class='u'
name="General_Usage"
>General Usage</a></h3>

<p>To specify what test(s) you wish to run, simply specify both its config and name together on the command line, as such:</p>

<pre>    verify config::name</pre>

<p>Verify will then search its database of tests for the test definition file containing this test. It will parse this file to gather any test information (such as test and tool parameters to use). This information will be passed to the build and run functions defined in the <em>build_test.pm</em> and <em>run_test.pm</em> files, which you will implement, with your build and run steps.</p>

<p>You may specify multiple tests to run in series by listing their config::name one after each other, separated with spaces, like so:</p>

<pre>    verify config1::test1 config2::test2 ...</pre>

<p>If you want to run the same test multiple times, you can use the repetition operator (<code>{}</code>) instead of specifying the full test name multiple times. For instance, to run <code>config1::test1</code> twice, you can type:</p>

<pre>    verify config1::test1{2}</pre>

<p>If you have test parameters, simply include them before the repetition operator:</p>

<pre>    verify config1::test1,param1{2}</pre>

<p>For more on test parameters, see the section called <b>Test Parameters</b>.</p>

<h3><a class='u'
name="Test_Parameters"
>Test Parameters</a></h3>

<p>You may pass parameters to the test itself by including them following the test name plus a comma, as such:</p>

<pre>    verify config::name,param1</pre>

<p>You can pass any number of parameters to the test itself, this way:</p>

<pre>    verify config::name,param1,param2,param3,param4</pre>

<p>These parameters need not fulfill any particular syntax, so long as spaces and other special characters are properly escaped. You may parse these arguments as you see fit in the pre/post/build and pre/post/run subroutines in <em>build_test.pm</em> and <em>run_test.pm</em>. Therefore, they are most useful for directly configuring test build and run behavior from the command-line.</p>

<h3><a class='u'
name="Running_in_Parallel_Mode"
>Running in Parallel Mode</a></h3>

<p>If you use the <code>--parallel</code> command-line option, you can specify a maximum number of tests allowed to run at once. This will cause Verify to run each test in its own separate process. At the same time, it will keep track of all currently running tests, logging test results and dispatching more as additional tests complete, until all tests have run. For example:</p>

<pre>    verify --parallel 2 config1::test1 config2::test2 config3::test3</pre>

<p>This executes all of the tests listed, while allowing for a maximum of 2 tests running in parallel. The Verify script will first run <code>config1::test1</code> and <code>config2::test2</code> in their own individual processes, which will be monitored by the main process. Once one of them completes, <code>config3::test3</code> will be dispatched. Once all tests complete, error status is logged, and the script exits. This means that at any given time, if running in parallel mode for a setting of N available parallel slots, you will at any given time have no more than N+1 processes executing, and no more than N tests running simultaneously. Test dispatch is first-come-first-serve, in the order listed on the command-line.</p>

<p>Once all tests complete, Verify will exit with an status code indicating pass or failure. A code of zero means &#39;pass&#39;. A code of non-zero means &#39;failed&#39;. Actual fail codes are determined by the user-implemented <em>build_test.pm</em> and <em>run_test.pm</em> files. The final error status is a bitwise OR of these error codes. The script will exit indicating a failure if this final error status is non-zero.</p>

<h2><a class='u'
name="Defining_Tests"
>Defining Tests</a></h2>

<h3><a class='u'
name="The_files"
>The files</a></h3>

<p>The files required when writing a test are:</p>

<ul>
<li><em>Your test source</em></li>

<li><em>testfile.test</em></li>
</ul>

<p>These files will be assumed to live in <em>$PRJ_HOME/verification/tests</em> by default. This can be redefined, however. See the subsection labelled <b>Installation and Configuration</b> for more information.</p>

<p>The <em>testfile.test</em> file is a simple text file containing the test definition. It has eight different keywords: <code>test...endtest</code>, <code>description</code>, <code>config</code>, <code>params</code>, <code>build.args</code>, <code>run.args</code>, <code>define</code>.</p>

<p><b>NOTE:</b> You do not have to have an individual <em>*.test</em> file per test, so long as there is a test definition associated with the test that exists in a <em>*.test</em> file located somewhere where Verify can find it.</p>

<h3><a class='u'
name="Descriptions_of_fields"
>Descriptions of fields</a></h3>

<dl>
<dt><a name="test...endtest"
><code>test</code>...<code>endtest</code></a></dt>

<dd>
<p>All test definitions are enclosed in a <code>test...endtest</code> block (henceforth referred to as &#34;test blocks&#34;). These test blocks also are used to define the name of the test. The syntax is:</p>

<pre>    test: test_name
      # Test info goes here
    endtest</pre>

<p>The <code>test_name</code> field is required.</p>

<dt><a name="description"
><code>description</code></a></dt>

<dd>
<p>A string containing a brief description of the test. This is a required field.</p>

<dt><a name="config"
><code>config</code></a></dt>

<dd>
<p>This corresponds to your testbench configuration associated with this test. This is a required field.</p>

<dt><a name="build.args"
><code>build.args</code></a></dt>

<dd>
<p>Contains a list of arguments to your build tool to be passed directly on the command-line. These arguments will not be parsed in any way, so be sure that they are well-formed!</p>

<dt><a name="run.args"
><code>run.args</code></a></dt>

<dd>
<p>Contains a list of arguments to your test binary to be passed directly on the command-line. These arguments will not be parsed in any way, so be sure that they are well-formed!</p>

<dt><a name="params"
><code>params</code></a></dt>

<dd>
<p>Contains a list of parameters to always use in addition to whatever is passed on the command-line as comma-separated test parameters.</p>

<dt><a name="define"
><code>define</code></a></dt>

<dd>
<p>(Also: <code>define build</code> and <code>define run</code>)</p>

<p>You can use this to specify custom parameters for the test, which can be passed on the command-line by appending a comma after the test name and listing the comma-separated list of parameters. You can also list them in the test file using the <code>params</code> line.</p>

<p>A <code>define</code> line in a test file looks like this:</p>

<pre>    define option1=+option1+1</pre>

<p>If you specify <code>option1</code> as a parameter when you invoke the test, <code>+option1+1</code> will then be appended to the build and run tools&#39; invocations using the <code>build.args</code> and <code>run.args</code> fields. Here&#39;s an example of how you invoke the test with this newly defined parameter:</p>

<pre>    verify.pl config::test,option1</pre>

<p>You can also create a define and specify that it should only apply to either the build tool or the run tool, like this:</p>

<pre>    define build option2=+option2+1
      or
    define run option2=+option2+1</pre>

<p>When using this method, <code>+option2+1</code> will only be applied to its corresponding tool when the test is invoked using the <code>option2</code> parameter.</p>

<p>You can also define parameters to accept strings. This will allow you to change build and run step behavior based on arbitrary values passed in when invoking the test. To do this, simply define your custom parameter as before, and use <code>$$</code> to represent the string to replace. For example, this will define a custom parameter that lets you set a plusarg to some arbitrary value:</p>

<pre>    define option3=+option3+$$</pre>

<p>Now, you can invoke the test and set the plusarg <code>+option3</code> to store any number or string you&#39;d like. To do this, invoke the test with the parameter as such:</p>

<pre>    verify.pl config::test,option3=15</pre>

<p>This will then pass <code>+option3+15</code> to your build and run tools as command-line arguments, setting the <code>+option3</code> plusarg to 15.</p>

<p>If you want to define a test parameter so that it will pass <code>$$</code> without substitution, simply escape it with a backslash (<code>\</code>):</p>

<pre>    define option4=+option4+\$$</pre>

<p>When that backslash is present, it will not substitute the <code>$$</code> out, and will pass the following to your build and run tools:</p>

<pre>    +option4+$$</pre>
</dd>
</dl>

<p><b>NOTE:</b> <em>Your test source</em> contains the code for your actual test. It may involve one or more files, depending on your verification methods and tool environment. Since the <em>build_test.pm</em> and <em>run_test.pm</em> files are user-implemented, identifying the test source may be done using values from one or more of these fields. Therefore, one must be conscious of the file and test naming conventions used.</p>

<h1><a class='u'
name="VERSIONS"
>VERSIONS</a></h1>

<ul>
<li><i>v.2.3.0, July 12, 2013</i>
<dl>
<dt><a name="Bugfixes:"
>Bugfixes:</a></dt>

<dd>
<ul>
<li>Fixed some bugs in TestIndexCSV where if a new testfile has been added to the file tree, it wouldn&#39;t be properly indexed.</li>
</ul>

<dt><a name="Enhancements:"
>Enhancements:</a></dt>

<dd>
<ul>
<li>Refactored test file parsing code, consolidating it into a module called TestFileParser.pm. This module simplifies test file reading and returns test file instructions using a standard format. TestIndex and TestIndexCSV now utilize this module for all test file parsing.</li>

<li>Aside from moving all test file parsing code to use the TestFileParser module, also did a lot of clean-up surrounding indexing code. Added some debug switches (accessible in the source code) to help with SQL statement debug.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v.2.2.2, June 17, 2013</i>
<dl>
<dt><a name="Features:"
>Features:</a></dt>

<dd>
<ul>
<li>Adding a new module that uses CSV files and SQL queries for test indexing. Supports indexing of additional metadata alongside test name and file (for instance, line number). This should be more useful for indexing tests in a few, very large .test files.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v2.2.1, June 13, 2013</i>
<dl>
<dt><a name="Enhancements:"
>Enhancements:</a></dt>

<dd>
<ul>
<li>Switched over from using a flat, sequential text file for the index, to using the Berkeley DB_FILE format and a BTree container structure. This should allow indexes to scale to larger sizes without introducing a significant performance penalty.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v2.2.0, May 13, 2013</i>
<dl>
<dt><a name="NOTE:"
>NOTE:</a></dt>

<dd>
<p>With this release, I&#39;m migrating the tool to the GitHub repository.</p>

<dt><a name="Bugfixes:"
>Bugfixes:</a></dt>

<dd>
<ul>
<li>Moved the checking whether $PRJ_HOME is set from compile-time to run-time so you don&#39;t have to set it just to view the help documentation.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v2.1.1, April 25, 2013</i>
<dl>
<dt><a name="Bugfixes:"
>Bugfixes:</a></dt>

<dd>
<ul>
<li>Fixed a bug where test parameters that pass a value to build and run might lose the value, especially when specified by using the <code>params</code> field in the test file.</li>
</ul>

<dt><a name="Features:"
>Features:</a></dt>

<dd>
<ul>
<li>Finally adding a <code>--version</code> switch with version information.</li>
</ul>

<dt><a name="Enhancements:"
>Enhancements:</a></dt>

<dd>
<ul>
<li>For log files, changed from using hard links and relative paths to symbolic links and absolute paths.</li>

<li>If a log file froma previous run is found it will now be backed up to verify.old.log before being overwritten.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v2.1, February 15, 2013</i>
<dl>
<dt><a name="Bugfixes:"
>Bugfixes:</a></dt>

<dd>
<ul>
<li>Fixed a bug where if you specify a test that doesn&#39;t exist in the index, the reindexing process would end up deleting the entire index and then rebuilding it from scratch every time.</li>

<li>Fixed a bug where if you ran a list of tests one after the other, the script would fail when trying to create log files for the second test in the list.</li>

<li>When running in parallel mode, if the build step failed for a test, it would still attempt to continue running the test, instead of properly exiting and continuing to the next test in the list.</li>

<li>Pass/Fail statuses reported when running with the <code>-n</code> switch contradicted each other (logging FAIL while logfile is named <em>pass*.log</em>). Fixed so now it&#39;s reported as a FAIL throughout the tool output.</li>

<li>When run with certain command-line switches that cause the tool to exit immediately (for instance, <code>--help</code> or <code>--print</code>), it would never generate the <em>verify_status.env</em> log of environment variables. Now it does.</li>
</ul>

<dt><a name="Features:"
>Features:</a></dt>

<dd>
<ul>
<li>Added the repetition operator so that you can tell the tool to repeat tests an arbitrary number of times without typing the full test name out more than once.</li>

<li>Added the <code>-r</code>/<code>--report</code> switch to enable pass/fail reports for the list of tests to be saved at run.</li>

<li>Added the <code>--man2html</code> switch for a more readable option for the manpage help.</li>
</ul>

<dt><a name="Enhancements:"
>Enhancements:</a></dt>

<dd>
<ul>
<li>Changed the <code>run_command()</code> implementation to use Perl IPCs instead of <code>system()</code> and appending tee to the file. This makes the logging mechanism a bit more robust.</li>

<li>Overhauled the formatting for the manpage help, for readability and to make the HTML generated help nicer as well.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v2.0, December 18, 2012</i>
<dl>
<dt><a name="Features:"
>Features:</a></dt>

<dd>
<ul>
<li>Overhauled the test parsing code, now implemented in a module called <em>TestIndexParser.pm</em>. This new parser updates the test definition file syntax and greatly increases its flexibility and scalability. We now support having multiple test definitions within a single <em>*.test</em> file, through the use of test blocks. Also, test files now may exist anywhere underneath the location specified by the <code>testsdir</code> configuration option, instead of being confined to <em><code>testsdir</code>/config/testname.test</em>. In order to reduce filesystem access time, this tool will also transparently maintain a test index, to allow quick and direct access to the associated test file when running a test.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v1.3, December 13, 2012</i>
<dl>
<dt><a name="Enhancements:"
>Enhancements:</a></dt>

<dd>
<ul>
<li>Extracted test parsing code into a separate module called <em>TestParser.pm</em>, and modified the tool to call this file. This is in preparation for a future revision allowing for greater enhancement of the test parsing and management system.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v1.2.6, November 29, 2012</i>
<dl>
<dt><a name="Bugfixes:"
>Bugfixes:</a></dt>

<dd>
<ul>
<li>Fixed a bug where if the tool failed when trying to include the <em>build_test.pm</em> and <em>run_test.pm</em> modules, it would fail silently, and cause the verify tool to fail. Now, the script will die with the specific, more descriptive error.</li>
</ul>

<dt><a name="Features:"
>Features:</a></dt>

<dd>
<ul>
<li>Extended the ability to configure the Verify tool using <em>verify.ini</em>. Now it will look for its configuration in <code>$PRJ_HOME</code>. It also now supports configuring where to find <em>build_test.pm</em> and <em>run_test.pm</em>. Instead of loading these files at compile time, it will load them at run time, which enables us to configure from where it should load them based on what we read in from the file.</li>

<li>Extended custom test parameters to allow passing in values. Now, any instance of double dollar signs (<code>$$</code>) will be replaced with anything you put to the right of the equals sign when passing the parameter on the command-line.</li>
</ul>

<dt><a name="Enhancements:"
>Enhancements:</a></dt>

<dd>
<ul>
<li>Updated the documentation to include a section describing how to install and configure the tool, as well as some basic information about implementing the build and run flows.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v1.2.5, November 20, 2012</i>
<dl>
<dt><a name="Bugfixes:"
>Bugfixes:</a></dt>

<dd>
<ul>
<li>Fixed enforcement of required fields in test files. The script now checks that they exist and will error if any are missing.</li>
</ul>

<dt><a name="Features:"
>Features:</a></dt>

<dd>
<ul>
<li>Added the ability to define test-specific parameter options from within the <em>*.test</em> files. You can list these on the command-line as comma-separated strings, or include them in the <code>params</code> line as default parameters in the <em>*.test</em> files themselves.</li>
</ul>

<dt><a name="Enhancements:"
>Enhancements:</a></dt>

<dd>
<ul>
<li>Since <code>--debug</code> and <code>--interactive</code> are supposed to set flags that are acted upon by the user code in build_test.pm and run_test.pm, this script now sets dedicated global flags, so the user doesn&#39;t have to use the <code>%options</code>.</li>

<li>In response to the previously listed enhancement, <code>%options</code> has now been made private to the main script file, instead of global. This will protect the script from being put into undefined configuration states through errant user code.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v1.2.1, October 26, 2012</i>
<dl>
<dt><a name="Bugfixes:"
>Bugfixes:</a></dt>

<dd>
<ul>
<li>Fixed a bug that caused zero-length files to accumulate when doing multiple runs over the same directory. Not a critical bug, but it eventually would run into the filesystem limit if left unchecked.</li>
</ul>

<dt><a name="Features:"
>Features:</a></dt>

<dd>
<ul>
<li>Added ability to specify directory where test files are stored using a configuration file.</li>
</ul>

<dt><a name="Enhancements:"
>Enhancements:</a></dt>

<dd>
<ul>
<li>Updated script logging for readability and to reflect paths where script and tests are stored.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v1.2, October 18, 2012</i>
<dl>
<dt><a name="Bugfixes:"
>Bugfixes:</a></dt>

<dd>
<ul>
<li>Fixed a bug where if the build step fails, log files weren&#39;t cleanly finished and links to the log files weren&#39;t created.</li>

<li>Removed some stale code for debug mode since it is no longer handled in the main script.</li>

<li>Fixed a bug exposed when running a list of tests, where if one or more tests fail while the last test in the list passes, the script will return with exit code &#39;0&#39; (pass). This causes the script to effectively forget about the error status of previous tests, and incorrectly report the test results.</li>

<li>Added enforcement of making the <code>-i</code>/<code>--interactive</code> command-line option override use of <code>-d</code>/<code>--debug</code> so that they are mutually exclusive.</li>
</ul>

<dt><a name="Features:"
>Features:</a></dt>

<dd>
<ul>
<li>Enhanced comment parsing in <em>*.test</em> files to allow you to include <code>#</code> characters by escaping them C-style (<code>\#</code>).</li>
</ul>

<dt><a name="Enhancements:"
>Enhancements:</a></dt>

<dd>
<ul>
<li>Updated help documentation with more descriptions on running tests and more organized formatting in VERSIONS section.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v1.1, August 28, 2012</i>
<dl>
<dt><a name="Bugfixes:"
>Bugfixes:</a></dt>

<dd>
<ul>
<li>Fixed a bug preventing errors to be logged to the error logfile.</li>
</ul>

<dt><a name="Features:"
>Features:</a></dt>

<dd>
<ul>
<li>Added additional test fields to allow for individual customization of build and run steps.</li>

<li>Added the ability to hard-code test parameters into test files as an alternative to passing them on the command-line.</li>

<li>Added version information to manpage-style documentation.</li>
</ul>
</dd>
</dl>
</li>

<li><i>v1.0, August 17, 2012</i>
<ul>
<li>Initial revision.</li>
</ul>
</li>
</ul>

<h1><a class='u'
name="AUTHORS"
>AUTHORS</a></h1>

<p>Benjamin D. Richards</p>

<h1><a class='u'
name="COPYRIGHT"
>COPYRIGHT</a></h1>

<p>This tool is licensed under the GNU GENERAL PUBLIC LICENSE Version 2 (GPLv2). See <em>COPYRIGHT</em> for its terms.</p>

<!-- end doc -->

</body></html>
